---
##Coursera Practical Machine Learning Final Project
#This document is an assingment for the practical machine learning course project, produced using RStudio’s Markdown and Knitr.
title: "Taisekwa Prediction assingment"
author: "Taisekwa Chikazhe"
date: "17 September 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The data consists of a Training data and a Test data (to be used to validate the selected model).

The goal of your project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. You may use any of the other variables to predict with.


# Loading required packages
```{r}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(corrplot)
library(gbm)
library(readr)
library(lattice)
library(tidyverse)
library(caret)
library(kernlab)
library(rattle)
library(corrplot)
```
# Load the required data
```{r}
traincsv <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testcsv <- read_csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

```
# check for data dimesions
```{r}
dim(traincsv)
dim(testcsv)

```
# Data cleaning
```{r}
traincsv <- traincsv[,colMeans(is.na(traincsv)) < .9] #removing mostly na columns
traincsv <- traincsv[,-c(1:7)] #removing metadata which is irrelevant to the outcome
dim(traincsv)

```
# Partitioning data, into training and testing data set
```{r}
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
train <- traincsv[inTrain,]
valid <- traincsv[-inTrain,]

```
# Ploting correlation plot
```{r}
corrPlot <- cor(train[, -length(names(train))])
corrplot(corrPlot, method="color")

```
# Creating and testing models
Here we will test a few popular models including: Decision Trees, Random Forest, Gradient Boosted Trees, and SVM.
```{r}
#Set up control for training to use 3-fold cross validation.
control <- trainControl(method="cv", number=3, verboseIter=F)
```
# Decision tree model
```{r}
mod_trees <- train(classe~., data=train, method="rpart", trControl = control, tuneLength = 5)
```
```{r}
fancyRpartPlot(mod_trees$finalModel)
```
#Prediction
```{r}
pred_trees <- predict(mod_trees, valid)
cmtrees <- confusionMatrix(pred_trees, factor(valid$classe))
cmtrees

```
# Decision tree cross validation
```{r}
plot(mod_trees)
```
# Random forest model
```{r}
mod_rf <- train(classe~., data=train, method="rf", trControl = control, tuneLength = 5)
```
# Random forest prediction
```{r}
pred_rf <- predict(mod_rf, valid)
cmrf <- confusionMatrix(pred_rf, factor(valid$classe))
cmrf

```
# Random forest cross validation
```{r}
plot(mod_rf)
```
# Gradient boosted trees model
```{r}
mod_gbm <- train(classe~., data=train, method="gbm", trControl = control, tuneLength = 5, verbose = F)
```
# Gradient boosted trees model prediction
```{r}
pred_gbm <- predict(mod_gbm, valid)
cmgbm <- confusionMatrix(pred_gbm, factor(valid$classe))
cmgbm

```
#GBM cross validation
```{r}
plot(mod_gbm)
```
# Support vector machine model
```{r}
mod_svm <- train(classe~., data=train, method="svmLinear", trControl = control, tuneLength = 5, verbose = F)
```
#Model prediction
```{r}
pred_svm <- predict(mod_svm, valid)
cmsvm <- confusionMatrix(pred_svm, factor(valid$classe))
cmsvm

```
#Results (Accuracy & Out of Sample Error)
     accuracy oos_error
Tree    0.537     0.463
RF      0.996     0.004
GBM     0.992     0.008
SVM     0.781     0.219
The best model is the Random Forest model, with 0.9957519 accuracy and 0.0042481 out of sample error rate.

